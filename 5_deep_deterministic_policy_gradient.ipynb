{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xt6fIDownZs"
      },
      "source": [
        "## Deep Deterministic Policy Gradient (DDPG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6RqrzokqoanP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install pytorch-lightning\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install brax\n",
        "!pip install gym==0.23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "B-Z6takfzqGk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f8b911a-be6f-4e47-f03d-4ec279e1952e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7d546638b010>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs.wrappers import gym as gym_wrapper\n",
        "from brax.envs.wrappers import torch as torch_wrapper\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "evrLpUqXKres"
      },
      "outputs": [],
      "source": [
        "def create_environment(env_name, num_envs=256, episode_length=1000):\n",
        "    env = envs.create(env_name, batch_size=num_envs, episode_length=episode_length, backend='spring')\n",
        "    env = gym_wrapper.VectorGymWrapper(env)\n",
        "    env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "    return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QwYlpTOY1Ajo"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def test_env(env_name, policy=None):\n",
        "  env = envs.create(env_name, episode_length=1000, backend='spring')\n",
        "  env = gym_wrapper.GymWrapper(env)\n",
        "  env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "  ps_array = []\n",
        "  state = env.reset()\n",
        "  for i in range(1000):\n",
        "    if policy:\n",
        "      action = algo.policy.net(state.unsqueeze(0)).squeeze()\n",
        "    else:\n",
        "      action = torch.from_numpy(env.action_space.sample()).to(device)\n",
        "    state, _, _, _ = env.step(action)\n",
        "    ps_array.extend([env.unwrapped._state.pipeline_state]*5)\n",
        "  return HTML(html.render(env.unwrapped._env.sys, ps_array))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training_epoch_end(self, outputs):\n",
        "#   if self.current_epoch % 1000 == 0:\n",
        "#     video = test_env('ant', policy=algo.policy)\n",
        "#     self.videos.append(video)\n",
        "\n",
        "# Where did he teach this?"
      ],
      "metadata": {
        "id": "ZE3exZS837w9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SmWkjyfs7kc"
      },
      "source": [
        "#### Create the gradient policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mNDbTLeZuP1m"
      },
      "outputs": [],
      "source": [
        "# Define a neural network-based policy using PyTorch - actor network in DDPG\n",
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, obs_size, out_dims, min, max):\n",
        "        \"\"\"\n",
        "        Initialize the Gradient Policy Network.\n",
        "\n",
        "        Args:\n",
        "            hidden_size (int): Number of neurons in hidden layers.\n",
        "            obs_size (int): Dimension of the input observation space.\n",
        "            out_dims (int): Dimension of the output action space.\n",
        "            min (np.ndarray): Minimum action values (for clamping).\n",
        "            max (np.ndarray): Maximum action values (for clamping).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Convert min and max action values to PyTorch tensors and move them to the appropriate device\n",
        "        self.min = torch.from_numpy(min).to(device)  # Lower bound for actions\n",
        "        self.max = torch.from_numpy(max).to(device)  # Upper bound for actions\n",
        "\n",
        "        # Define a simple feedforward neural network with two hidden layers\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),  # First hidden layer\n",
        "            nn.ReLU(),  # Activation function (ReLU)\n",
        "            nn.Linear(hidden_size, hidden_size),  # Second hidden layer\n",
        "            nn.ReLU(),  # Activation function (ReLU)\n",
        "            nn.Linear(hidden_size, out_dims),  # Output layer (raw action values)\n",
        "            nn.Tanh()  # Output activation (Tanh to keep values in [-1, 1])\n",
        "        )\n",
        "\n",
        "    def mu(self, x):\n",
        "        \"\"\"\n",
        "        Compute the mean action for a given state.\n",
        "\n",
        "        Args:\n",
        "            x (np.ndarray or torch.Tensor): Input state (observation).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Scaled action values in the valid range.\n",
        "        \"\"\"\n",
        "        # If input is a NumPy array, convert it to a PyTorch tensor\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.from_numpy(x).to(device)\n",
        "\n",
        "        # Forward pass through the network and scale the output to the max action range\n",
        "        return self.net(x.float()) * self.max  # Scale by max to map [-1,1] range to actual action range\n",
        "\n",
        "    def forward(self, x, epsilon=0.0):\n",
        "        \"\"\"\n",
        "        Compute the action with optional exploration noise.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor or np.ndarray): Input state (observation).\n",
        "            epsilon (float, optional): Standard deviation of Gaussian exploration noise. Default is 0.0.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Clamped action values within the defined min/max range.\n",
        "        \"\"\"\n",
        "        mu = self.mu(x)  # Compute the mean action from the policy network\n",
        "\n",
        "        # Add Gaussian exploration noise with mean 0 and standard deviation epsilon\n",
        "        mu = mu + torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "\n",
        "        # Clip the action within the allowed range [min, max] to ensure valid actions\n",
        "        action = torch.max(torch.min(mu, self.max), self.min)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dv2XzwmtB3r"
      },
      "source": [
        "#### Create the Deep Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k9Z8OviE8V1A"
      },
      "outputs": [],
      "source": [
        "# Define a Deep Q-Network (DQN) - used as the Critic in DDPG\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, obs_size, out_dims):\n",
        "        \"\"\"\n",
        "        Initialize the Deep Q-Network (Critic).\n",
        "\n",
        "        Args:\n",
        "            hidden_size (int): Number of neurons in hidden layers.\n",
        "            obs_size (int): Dimension of the input state space.\n",
        "            out_dims (int): Dimension of the action space.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Define a neural network to approximate the Q-value function Q(s, a)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size + out_dims, hidden_size),  # Input layer (state + action)\n",
        "            nn.ReLU(),  # Activation function (ReLU)\n",
        "            nn.Linear(hidden_size, hidden_size),  # Hidden layer\n",
        "            nn.ReLU(),  # Activation function (ReLU)\n",
        "            nn.Linear(hidden_size, 1)  # Output layer (single Q-value)\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        \"\"\"\n",
        "        Compute the Q-value for a given state-action pair.\n",
        "\n",
        "        Args:\n",
        "            state (torch.Tensor or np.ndarray): The state input.\n",
        "            action (torch.Tensor or np.ndarray): The action input.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Estimated Q-value for the given (state, action) pair.\n",
        "        \"\"\"\n",
        "        # Convert NumPy arrays to PyTorch tensors if necessary\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).to(device)\n",
        "        if isinstance(action, np.ndarray):\n",
        "            action = torch.from_numpy(action).to(device)\n",
        "\n",
        "        # Concatenate state and action into a single input vector\n",
        "        in_vector = torch.hstack((state, action))  # Horizontal stacking of tensors\n",
        "\n",
        "        # Pass the input through the neural network and return the Q-value\n",
        "        return self.net(in_vector.float())  # Ensure the input is a float tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8XxdqufquQK6"
      },
      "outputs": [],
      "source": [
        "# Experience Replay Buffer\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "      self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "      return random.sample(self.buffer, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "roQfFswgKAZC"
      },
      "outputs": [],
      "source": [
        "# DataLoader for the experience replay buffer\n",
        "class RLDataset(IterableDataset):\n",
        "    def __init__(self, buffer, sample_size=400):\n",
        "        self.buffer = buffer\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for experience in self.buffer.sample(self.sample_size):\n",
        "            yield experience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O7URcCS7uQNc"
      },
      "outputs": [],
      "source": [
        "# Polyak averaging function for updating target networks\n",
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1.0 - tau) * tp.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "KtxZePn-uQQA"
      },
      "outputs": [],
      "source": [
        "from os import access\n",
        "# Define the DDPG algorithm using PyTorch Lightning for training automation\n",
        "class DDPG(LightningModule):\n",
        "\n",
        "    def __init__(self, env_name, capacity=500, batch_size=8192, actor_lr=1e-3,\n",
        "                 critic_lr=1e-3, hidden_size=256, gamma=0.99, loss_fn=F.smooth_l1_loss,\n",
        "                 optim=AdamW, eps_start=1.0, eps_end=0.2, eps_last_episode=500,\n",
        "                 samples_per_epoch=10, tau=0.005):\n",
        "        \"\"\"\n",
        "        Initialize the DDPG agent.\n",
        "\n",
        "        Args:\n",
        "            env_name (str): Name of the environment to train in.\n",
        "            capacity (int): Size of the experience replay buffer.\n",
        "            batch_size (int): Number of samples per training batch.\n",
        "            actor_lr (float): Learning rate for the actor network.\n",
        "            critic_lr (float): Learning rate for the critic network.\n",
        "            hidden_size (int): Number of neurons in hidden layers.\n",
        "            gamma (float): Discount factor for future rewards.\n",
        "            loss_fn: Loss function for the critic (default: smooth L1 loss).\n",
        "            optim: Optimizer type (default: AdamW).\n",
        "            eps_start (float): Initial exploration noise.\n",
        "            eps_end (float): Final exploration noise.\n",
        "            eps_last_episode (int): Number of episodes over which exploration decays.\n",
        "            samples_per_epoch (int): Number of samples to collect per training epoch.\n",
        "            tau (float): Soft update factor for target networks.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Create the environment with multiple parallel instances (batch learning)\n",
        "        self.env = create_environment(env_name, num_envs=batch_size)\n",
        "\n",
        "        # Reset the environment and store the initial observation\n",
        "        self.obs = self.env.reset()\n",
        "\n",
        "        # Video storage for visualization (optional)\n",
        "        self.videos = []\n",
        "\n",
        "        # Get state (observation) and action dimensions from the environment\n",
        "        self.obs_size = self.env.observation_space.shape[1]  # State space dimension\n",
        "        self.action_size = self.env.action_space.shape[1]  # Action space dimension\n",
        "\n",
        "        # Initialize the Critic network (Q-value function)\n",
        "        self.qnet = DQN(hidden_size, self.obs_size, self.action_size)\n",
        "\n",
        "        # Initialize the Actor network (policy function)\n",
        "        self.policy = GradientPolicy(hidden_size, self.obs_size, self.action_size,\n",
        "                                     self.env.action_space.low, self.env.action_space.high)\n",
        "\n",
        "        # Create target networks (used for stable learning)\n",
        "        self.target_qnet = copy.deepcopy(self.qnet)  # Target Critic\n",
        "        self.target_policy = copy.deepcopy(self.policy)  # Target Actor\n",
        "\n",
        "        # Experience Replay Buffer (stores past experiences for training)\n",
        "        self.buffer = ReplayBuffer(capacity)\n",
        "\n",
        "        # Save hyperparameters for logging and checkpointing\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Fill the replay buffer with initial experience\n",
        "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "            print(f'Filling replay buffer: {len(self.buffer)}/{self.hparams.samples_per_epoch}')\n",
        "            self.play(epsilon=self.hparams.eps_start)  # Play an episode with initial exploration noise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play(self, policy=None, epsilon=0.0):\n",
        "      \"\"\"\n",
        "      Executes one step in the environment and stores the experience in the replay buffer.\n",
        "\n",
        "      Args:\n",
        "          policy (callable, optional): The policy function to select actions. If None, selects a random action.\n",
        "          epsilon (float): Exploration noise to add to the policy's action.\n",
        "\n",
        "      Returns:\n",
        "          float: The mean reward obtained in this step.\n",
        "      \"\"\"\n",
        "\n",
        "      # Select an action using the given policy or randomly sample one\n",
        "      if policy:\n",
        "          action = policy(self.obs, epsilon=epsilon)  # Get action from the given policy with exploration noise\n",
        "      else:\n",
        "          action = torch.from_numpy(self.env.action_space.sample()).to(device)  # Take a random action\n",
        "\n",
        "      # Execute the chosen action in the environment\n",
        "      next_obs, reward, done, info = self.env.step(action)\n",
        "\n",
        "      # Store the transition (state, action, reward, next state, done) in the replay buffer\n",
        "      experience = (self.obs, action, reward, next_obs, done)\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "      # Update the current observation for the next step\n",
        "      self.obs = next_obs\n",
        "\n",
        "      # Return the mean reward across all parallel environments\n",
        "      return reward.mean()\n",
        "\n",
        "\n",
        "    def forward(self, obs):\n",
        "        \"\"\"\n",
        "        Forward pass for the actor network (policy network).\n",
        "\n",
        "        Args:\n",
        "            obs (torch.Tensor): The current state/observation from the environment.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The action selected by the policy network.\n",
        "        \"\"\"\n",
        "        output = self.policy.mu(obs)  # Compute the action using the actor network\n",
        "        return output\n",
        "\n",
        "\n",
        "    def configure_optimisers(self):\n",
        "        \"\"\"\n",
        "        Configures optimizers for both the critic (Q-network) and actor (policy network).\n",
        "\n",
        "        Returns:\n",
        "            list: A list containing the optimizers for the Q-network and the policy network.\n",
        "        \"\"\"\n",
        "        # Create an optimizer for the Q-network (critic)\n",
        "        qnet_optimiser = self.hparams.optim(self.qnet.parameters(), lr=self.hparams.critic_lr)\n",
        "\n",
        "        # Create an optimizer for the policy network (actor)\n",
        "        policy_optimiser = self.hparams.optim(self.policy.parameters(), lr=self.hparams.actor_lr)\n",
        "\n",
        "        # Return both optimizers\n",
        "        return [qnet_optimiser, policy_optimiser]\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        \"\"\"\n",
        "        Creates a DataLoader to sample training data from the replay buffer.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: A PyTorch DataLoader that fetches experience batches from the replay buffer.\n",
        "        \"\"\"\n",
        "        return DataLoader(\n",
        "            RLDataset(self.buffer, self.hparams.batch_size),  # Dataset that wraps the replay buffer\n",
        "            batch_size=1  # Each batch fetches a single batch of experiences from the dataset\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        \"\"\"\n",
        "        Performs a single training step for the Deep Deterministic Policy Gradient (DDPG) agent.\n",
        "\n",
        "        This function updates either the critic (Q-network) or the actor (policy network),\n",
        "        depending on the optimizer index.\n",
        "\n",
        "        Args:\n",
        "            batch (tuple): A batch of experiences (states, actions, rewards, next_states, dones).\n",
        "            batch_idx (int): Index of the batch (not used explicitly).\n",
        "            optimizer_idx (int): Determines whether to update the Q-network (0) or policy network (1).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The loss value for either the Q-network or the policy network.\n",
        "        \"\"\"\n",
        "\n",
        "        # Epsilon decay for exploration (used in training to control noise level)\n",
        "        epsilon = max(\n",
        "            self.hparams.eps_end,\n",
        "            self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "        )\n",
        "\n",
        "        # Play an episode using the policy and log the mean reward\n",
        "        mean_reward = self.play(policy=self.policy.mu, epsilon=epsilon)\n",
        "        self.log('episode/mean_reward', mean_reward, prog_bar=True)\n",
        "\n",
        "        # Soft update (Polyak averaging) of the target networks\n",
        "        polyak_average(self.policy.net, self.target_policy.net, self.hparams.tau)\n",
        "        polyak_average(self.qnet, self.target_qnet, self.hparams.tau)\n",
        "\n",
        "        # Unpack batch: Convert batch tensors from shape (batch_size, 1, ...) to (batch_size, ...)\n",
        "        states, actions, rewards, next_states, dones = map(torch.squeeze, batch)\n",
        "\n",
        "        # Ensure rewards and dones are in the correct shape\n",
        "        rewards = rewards.unsqueeze(1)  # Convert to (batch_size, 1)\n",
        "        dones = dones.unsqueeze(1).bool()  # Convert to boolean tensor\n",
        "\n",
        "        # If optimizer_idx is 0, update the Q-network (Critic)\n",
        "        if optimizer_idx == 0:\n",
        "            # Compute Q-values for current states and actions\n",
        "            q_values = self.qnet(states, actions)\n",
        "\n",
        "            # Compute next Q-values using the target Q-network and target policy\n",
        "            next_q_values = self.target_qnet(next_states, self.target_policy.mu(next_states))\n",
        "\n",
        "            # Compute target Q-values using Bellman equation:\n",
        "            # Q_target = reward + Î³ * Q_next * (1 - done)\n",
        "            target_q_values = rewards + self.hparams.gamma * next_q_values * (1 - dones)\n",
        "\n",
        "            # Compute Q-loss (difference between predicted and target Q-values)\n",
        "            qloss = self.hparams.loss_fn(q_values, target_q_values)\n",
        "\n",
        "            # Log the Q-loss for monitoring\n",
        "            self.log('loss/q_loss', qloss)\n",
        "\n",
        "            return qloss  # Return Q-loss for optimization\n",
        "\n",
        "        # If optimizer_idx is 1, update the policy network (Actor)\n",
        "        elif optimizer_idx == 1:\n",
        "            # Compute actions using the current policy\n",
        "            mu = self.policy.mu(states)\n",
        "\n",
        "            # Compute policy loss (negative of expected Q-values, as we maximize Q)\n",
        "            policy_loss = -self.qnet(states, mu).mean()\n",
        "\n",
        "            # Log the policy loss for monitoring\n",
        "            self.log('loss/policy_loss', policy_loss)\n",
        "\n",
        "            return policy_loss  # Return policy loss for optimization\n",
        "\n",
        "\n",
        "    def train_epoch_end(self, outputs):\n",
        "      if self.current_epoch % 1000 == 0:\n",
        "        video = test_env('ant', policy=algo.policy)\n",
        "        self.videos.append(video)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhveWEKiwdy-"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr82W3E0uQSo"
      },
      "outputs": [],
      "source": [
        "algo = DDPG('brax-ant-v0')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBTeij99B0Bf"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    gpus=num_gpus,\n",
        "    max_epochs=5000,\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXVLEDQj2vhb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}