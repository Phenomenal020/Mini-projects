{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StmTP2EXwnUr"
      },
      "source": [
        "## Twin Delayed DDPG (TD3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5s0R3qxBnTN"
      },
      "outputs": [],
      "source": [
        "## Twin Delayed DDPG (TD3)\n",
        "%%capture\n",
        "\n",
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install gym\n",
        "!pip install pytorch-lightning\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install brax\n",
        "\n",
        "#### Setup virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()\n",
        "\n",
        "#### Import the necessary code libraries\n",
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import functools\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs.wrappers import gym as gym_wrapper\n",
        "from brax.envs.wrappers import torch as torch_wrapper\n",
        "\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()\n",
        "\n",
        "def display_video(episode=0):\n",
        "    video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "    video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "    return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")\n",
        "\n",
        "def create_environment(env_name, num_envs=256, episode_length=1000):\n",
        "    env = envs.create(env_name, batch_size=num_envs, episode_length=episode_length, backend='spring')\n",
        "    env = gym_wrapper.VectorGymWrapper(env)\n",
        "    env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "    return env\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_env(env_name, policy=None):\n",
        "    env = envs.create(env_name, episode_length=1000, backend='spring')\n",
        "    env = gym_wrapper.GymWrapper(env)\n",
        "    env = torch_wrapper.TorchWrapper(env, device=device)\n",
        "    ps_array = []\n",
        "    state = env.reset()\n",
        "    for i in range(1000):\n",
        "        if policy:\n",
        "            action = algo.policy.net(state.unsqueeze(0)).squeeze()\n",
        "        else:\n",
        "            action = torch.from_numpy(env.action_space.sample()).to(device)\n",
        "        state, _, _, _ = env.step(action)\n",
        "        ps_array.extend([env.unwrapped._state.pipeline_state]*5)\n",
        "    return HTML(html.render(env.unwrapped._env.sys, ps_array))\n",
        "test_env('ant')\n",
        "\n",
        "#### Create the gradient policy\n",
        "class GradientPolicy(nn.Module):\n",
        "    def __init__(self, hidden_size, obs_size, out_dims, min, max):\n",
        "        super().__init__()\n",
        "        self.min = torch.from_numpy(min).to(device)\n",
        "        self.max = torch.from_numpy(max).to(device)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, out_dims),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def mu(self, x):\n",
        "        if isinstance(x, np.ndarray):\n",
        "            x = torch.from_numpy(x).to(device)\n",
        "        return self.net(x.float()) * self.max\n",
        "\n",
        "\n",
        "    # NOISE CLIP 1\n",
        "    def forward(self, x, epsilon=0.0, noise_clip=None):\n",
        "        mu = self.mu(x)\n",
        "        noise = torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "        if noise_clip is not None:\n",
        "            noise = torch.clamp(noise, - noise_clip, noise_clip)\n",
        "        mu = mu + noise\n",
        "        action = torch.max(torch.min(mu, self.max), self.min)\n",
        "        return action\n",
        "\n",
        "\n",
        "\n",
        "#### Create the Deep Q-Network\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, hidden_size, obs_size, out_dims):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size + out_dims, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        if isinstance(state, np.ndarray):\n",
        "            state = torch.from_numpy(state).to(device)\n",
        "        if isinstance(action, np.ndarray):\n",
        "            action = torch.from_numpy(action).to(device)\n",
        "        in_vector = torch.hstack((state, action))\n",
        "        return self.net(in_vector.float())\n",
        "\n",
        "#### Create the Replay Buffer\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "\n",
        "#### Create a DatasetLoader\n",
        "class RLDataset(IterableDataset):\n",
        "    def __init__(self, buffer, sample_size=400):\n",
        "        self.buffer = buffer\n",
        "        self.sample_size = sample_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for experience in self.buffer.sample(self.sample_size):\n",
        "            yield experience\n",
        "\n",
        "#### Define polyak averaging for target network updates\n",
        "def polyak_average(net, target_net, tau=0.01):\n",
        "    for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "        tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)\n",
        "\n",
        "\n",
        "#### Create the Deep Q-Learning\n",
        "class TD3(LightningModule):\n",
        "\n",
        "    def __init__(self, env_name, capacity=500, batch_size=8192, actor_lr=1e-3,\n",
        "                 critic_lr=1e-3, hidden_size=256, gamma=0.99, loss_fn=F.smooth_l1_loss,\n",
        "                 optim=AdamW, eps_start=1.0, eps_end=0.2, eps_last_episode=500,\n",
        "                 samples_per_epoch=10, tau=0.005):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.env = create_environment(env_name, num_envs=batch_size)\n",
        "        self.obs = self.env.reset()\n",
        "        self.videos = []\n",
        "\n",
        "        obs_size = self.env.observation_space.shape[1]\n",
        "        action_dims = self.env.action_space.shape[1]\n",
        "        max_action = self.env.action_space.high\n",
        "        min_action = self.env.action_space.low\n",
        "\n",
        "        # TWIN 1\n",
        "        self.q_net1 = DQN(hidden_size, obs_size, action_dims).to(device)\n",
        "        self.q_net2 = DQN(hidden_size, obs_size, action_dims).to(device)\n",
        "        self.policy = GradientPolicy(hidden_size, obs_size, action_dims, min_action, max_action).to(device)\n",
        "\n",
        "        self.target_q_net1 = copy.deepcopy(self.q_net1)\n",
        "        self.target_q_net2 = copy.deepcopy(self.q_net2)\n",
        "        self.target_policy = copy.deepcopy(self.policy)\n",
        "\n",
        "        self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "            print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "            self.play(epsilon=self.hparams.eps_start)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play(self, policy=None, epsilon=0.):\n",
        "        if policy:\n",
        "            action = policy(self.obs, epsilon=epsilon)\n",
        "        else:\n",
        "            action = torch.from_numpy(self.env.action_space.sample()).to(device)\n",
        "        next_obs, reward, done, info = self.env.step(action)\n",
        "        exp = (self.obs, action, reward, done, next_obs)\n",
        "        self.buffer.append(exp)\n",
        "        self.obs = next_obs\n",
        "        return reward.mean()\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.policy.mu(x)\n",
        "        return output\n",
        "\n",
        "    # TWIN 2\n",
        "    def configure_optimizers(self):\n",
        "        q_net_parameters = itertools.chain(self.q_net1.parameters(), self.q_net2.parameters())\n",
        "        q_net_optimizer = self.hparams.optim(q_net_parameters, lr=self.hparams.critic_lr)\n",
        "        policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.actor_lr)\n",
        "        return [q_net_optimizer, policy_optimizer]\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "        dataloader = DataLoader(\n",
        "            dataset=dataset,\n",
        "            batch_size=1\n",
        "        )\n",
        "        return dataloader\n",
        "\n",
        "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "        epsilon = max(\n",
        "            self.hparams.eps_end,\n",
        "            self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode\n",
        "        )\n",
        "\n",
        "        mean_reward = self.play(policy=self.policy, epsilon=epsilon)\n",
        "        self.log('episode/mean_reward', mean_reward)\n",
        "\n",
        "        # TWIN 5\n",
        "        polyak_average(self.q_net1, self.target_q_net1, tau=self.hparams.tau)\n",
        "        polyak_average(self.q_net2, self.target_q_net2, tau=self.hparams.tau)\n",
        "        polyak_average(self.policy, self.target_policy, tau=self.hparams.tau)\n",
        "\n",
        "        states, actions, rewards, dones, next_states = map(torch.squeeze, batch)\n",
        "        rewards = rewards.unsqueeze(1)\n",
        "        dones = dones.unsqueeze(1).bool()\n",
        "\n",
        "        if optimizer_idx == 0:\n",
        "            # TWIN 4\n",
        "            action_values1 = self.q_net1(states, actions)\n",
        "            action_values2 = self.q_net2(states, actions)\n",
        "\n",
        "            # NOISE CLIP 2\n",
        "            next_actions = self.target_policy(next_states, epsilon=epsilon, noise_clip=0.05)\n",
        "\n",
        "            next_action_values = torch.min(\n",
        "                self.target_q_net1(next_states, next_actions),\n",
        "                self.target_q_net2(next_states, next_actions),\n",
        "            )\n",
        "            next_action_values[dones] = 0.0\n",
        "\n",
        "            expected_action_values = rewards + self.hparams.gamma * next_action_values\n",
        "\n",
        "            q_loss1 = self.hparams.loss_fn(action_values1, expected_action_values)\n",
        "            q_loss2 = self.hparams.loss_fn(action_values2, expected_action_values)\n",
        "            total_loss = q_loss1 + q_loss2\n",
        "            self.log(\"episode/Q-Loss\", total_loss)\n",
        "            return total_loss\n",
        "\n",
        "        # DELAYED: Only update the policy half the time\n",
        "        elif optimizer_idx == 1 and batch_idx % 2 == 0:\n",
        "            mu = self.policy.mu(states)\n",
        "            # TWIN 3\n",
        "            policy_loss = - self.q_net1(states, mu).mean()\n",
        "            self.log(\"episode/Policy Loss\", policy_loss)\n",
        "            return policy_loss\n",
        "\n",
        "    def training_epoch_end(self, training_step_outputs):\n",
        "        if self.current_epoch % 1000 == 0:\n",
        "            video = test_env('ant', policy=self.policy)\n",
        "\n",
        "#### Start tensorboard.\n",
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/\n",
        "\n",
        "algo = TD3('ant')\n",
        "\n",
        "trainer = Trainer(\n",
        "    gpus=num_gpus,\n",
        "    max_epochs=5_000,\n",
        "    log_every_n_steps=10\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}